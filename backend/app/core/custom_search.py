import requests
import os
import logging 
from elasticsearch import Elasticsearch
from typing import List, Dict, Any, Optional
from transformers import AutoTokenizer, AutoModel
import torch

model_name = "sentence-transformers/all-MiniLM-L6-v2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

def fetch_custom_search_results(query: str, num_results: int = 10) -> list[dict]:
    """
    Query the Google API Custom Search JSON and get the results.

    Args:
        query: Search query
        num_results: Number of results to return (maximum 10)

    Returns:
        list[dict]: List of results with relevant information
    """
    api_key = os.getenv("GOOGLE_API_KEY")
    search_engine_id = os.getenv("SEARCH_ENGINE_ID")
    url = "https://www.googleapis.com/customsearch/v1"

    params = {
        "q": query,
        "key": api_key,
        "cx": search_engine_id,
        "num": num_results
    }

    response = requests.get(url, params=params)
    if response.status_code == 200:
        results = response.json().get("items", [])
        return results
    else:
        raise Exception(f"Error querying the API: {response.status_code}, {response.text}")

def generate_embedding(text: str) -> list[float]:
    """
    Generates an embedding for a given text using Hugging Face.

    Args:
        text: The text to process.

    Returns:
        list[float]: Text embedding as a list of floats.
    """
    # Tokenize the text
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)

    # Generate model representations
    with torch.no_grad():
        outputs = model(**inputs)

    # Extract the [CLS] token representation
    # Typically, the [CLS] token embedding (position 0) is representative of the entire text
    embedding = outputs.last_hidden_state[:, 0, :].squeeze().tolist()
    return embedding

def process_search_results(results: list[dict]) -> list[dict]:
    """
    Processes the results from the Custom Search JSON API and prepares them for Elasticsearch.
    Generates embeddings using Hugging Face.

    Args:
        results: List of raw API results.

    Returns:
        list[dict]: List of processed documents.
    """
    documents = []
    for item in results:
        # Combined text from the title and snippet
        text = f"{item.get('title', '')} {item.get('snippet', '')}".strip()

        # Generate the embedding using Hugging Face
        vector = generate_embedding(text)

        # Create the processed document
        documents.append({
            "title": item.get("title", ""),
            "author": "Google Search",
            "publication_date": None,
            "abstract": item.get("snippet", ""),
            "keywords": [],
            "content": item.get("link", ""),
            "vector": vector  # Vector generated by the model
        })
    return documents

def vector_text_search(
    client: Elasticsearch,
    index_name: str,
    query_text: str,
    query_vector: List[float],
    min_score: float = 0.1,
    size: int = 10
) -> List[Dict[str, Any]]:
    """
    Performs a combined search by text and vector similarity.

    Args:
        client: Elasticsearch client
        index_name: Index name
        query_text: Text for search
        query_vector: Vector for search
        min_score: Minimum score to filter results
        size: Maximum number of results

    Returns:
        List[Dict]: List of found documents
    """
    try:
        query = {
            "size": size,
            "query": {
                "script_score": {
                    "query": {
                        "multi_match": {
                            "query": query_text,
                            "fields": ["title^3", "abstract^2", "content"],
                            "fuzziness": "AUTO"
                        }
                    },
                    "script": {
                        "source": """
                            cosineSimilarity(params.query_vector, 'vector') + 1.0 + 
                            (doc['keywords'].size() > 0 ? 0.5 : 0)
                        """,
                        "params": {"query_vector": query_vector}
                    }
                }
            }
        }

        response = client.search(index=index_name, body=query)
        
        results = []
        for hit in response['hits']['hits']:
            result = {
                'id': hit['_id'],
                'score': hit['_score'],
                'title': hit['_source'].get('title', ''),
                'abstract': hit['_source'].get('abstract', ''),
                'author': hit['_source'].get('author', ''),
                'publication_date': hit['_source'].get('publication_date'),
                'keywords': hit['_source'].get('keywords', []),
                'content': hit['_source'].get('content', '')
            }
            results.append(result)

        logging.info(f"Search completed. Found {len(results)} results")
        return results

    except Exception as e:
        logging.error(f"Error in search: {str(e)}")
        return []

def advanced_search(
    client: Elasticsearch,
    index_name: str,
    title: Optional[str] = None,
    author: Optional[str] = None,
    date_from: Optional[str] = None,
    date_to: Optional[str] = None,
    keywords: Optional[List[str]] = None,
    content: Optional[str] = None,
    size: int = 10
) -> List[Dict[str, Any]]:
    """
    Performs an advanced search with multiple criteria.

    Args:
        client: Elasticsearch client
        index_name: Index name
        title: Text to search in the title
        author: Specific author
        date_from: Start date (format: YYYY-MM-DD)
        date_to: End date (format: YYYY-MM-DD)
        keywords: List of keywords
        content: Text to search in the content
        size: Maximum number of results

    Returns:
        List[Dict]: List of found documents
    """
    try:
        must_conditions = []
        
        if title:
            must_conditions.append({
                "match": {
                    "title": {
                        "query": title,
                        "fuzziness": "AUTO"
                    }
                }
            })
            
        if author:
            must_conditions.append({
                "term": {
                    "author.keyword": author
                }
            })
            
        if date_from or date_to:
            must_conditions.append({
                "range": {
                    "publication_date": {
                        "gte": date_from,
                        "lte": date_to,
                        "format": "yyyy-MM-dd"
                    }
                }
            })
            
        if keywords:
            must_conditions.append({
                "terms": {
                    "keywords": keywords
                }
            })
            
        if content:
            must_conditions.append({
                "match": {
                    "content": {
                        "query": content,
                        "fuzziness": "AUTO"
                    }
                }
            })

        query = {
            "size": size,
            "query": {
                "bool": {
                    "must": must_conditions if must_conditions else [{"match_all": {}}]
                }
            },
            "sort": [
                {"_score": "desc"},
                {"publication_date": {"order": "desc", "missing": "_last"}}
            ]
        }

        response = client.search(index=index_name, body=query)
        
        results = []
        for hit in response['hits']['hits']:
            results.append(hit['_source'])

        logging.info(f"Advanced search completed. Found {len(results)} results")
        return results

    except Exception as e:
        logging.error(f"Error in advanced search: {str(e)}")
        return []
